{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":58130,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":48728}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Tensorflow**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Activation, Dropout, Conv2D, ZeroPadding2D, LayerNormalization, GlobalAveragePooling2D, Reshape, GlobalAveragePooling1D\nimport collections.abc\nfrom itertools import repeat\nimport numpy as np\n\n# Helper functions\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable) and not isinstance(x, str):\n            return tuple(x)\n        return tuple(repeat(x, n))\n    return parse\n\nto_2tuple = _ntuple(2)\n\nclass Identity(tf.keras.layers.Layer):\n    def __init__(self):\n        super().__init__()\n\n    def call(self, x):\n        return x\n\nclass DropPath(tf.keras.layers.Layer):\n    def __init__(self, drop_prob=0., scale_by_keep=True):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n        self.keep_prob = 1 - drop_prob\n        self.scale_by_keep = scale_by_keep\n\n    def sampling(self, x, shape, prob):\n        if not shape[0]:\n            return x\n        random_tensor = tf.random.uniform(shape=shape, minval=0, maxval=1)\n        random_tensor = tf.where(random_tensor < prob, 1., 0.)\n        if prob > 0.0 and self.scale_by_keep:\n            random_tensor = random_tensor / prob\n        return x * tf.cast(random_tensor, dtype=x.dtype)\n\n    def drop_path(self, x, drop_prob=0., training=False, scale_by_keep=True):\n        if drop_prob == 0. or not training:\n            return x\n        keep_prob = 1 - drop_prob\n        shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n        random_tensor = self.sampling(x, shape, keep_prob)\n        return random_tensor\n\n    def call(self, x, training=None):\n        return self.drop_path(x, self.drop_prob, training, self.scale_by_keep)\n\nclass PatchEmbed(tf.keras.layers.Layer):\n    def __init__(self, img_size=(224, 224), patch_size=4, in_chans=3, embed_dim=96, use_conv_embed=False, norm_layer=None, is_stem=False):\n        super().__init__()\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        if use_conv_embed:\n            if is_stem:\n                kernel_size = 7\n                padding = 2\n                stride = 4\n            else:\n                kernel_size = 3\n                padding = 1\n                stride = 2\n\n            self.pad_proj = ZeroPadding2D(padding=padding)\n            self.proj = Conv2D(filters=embed_dim, kernel_size=kernel_size, strides=stride, padding='valid', name=\"proj_conv\")\n        else:\n            self.pad_proj = Identity()\n            self.proj = Conv2D(filters=embed_dim, kernel_size=patch_size, strides=patch_size, padding='valid', name=\"proj_conv\")\n\n        if norm_layer is not None:\n            self.norm = norm_layer(epsilon=1e-5)\n        else:\n            self.norm = None\n        self.flatten = Reshape(target_shape=(-1, embed_dim))\n\n    def call(self, x):\n        B, H, W, C = x.shape\n\n        x = self.pad_proj(x)\n        x = self.proj(x)\n        \n        B, H, W, C = x.shape\n        x = self.flatten(x)\n        if self.norm is not None:\n            x = self.norm(x)\n        return x, H, W\n\nclass Mlp(tf.keras.layers.Layer):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=\"gelu\", drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = Dense(hidden_features, name=\"fc1\")\n        self.act = Activation(act_layer, name=\"act\")\n        self.fc2 = Dense(out_features, name=\"fc2\")\n        self.drop = Dropout(drop, name=\"drop\")\n\n    def call(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\nclass FocalModulation(tf.keras.layers.Layer):\n    def __init__(self, dim, focal_window, focal_level, focal_factor=2, bias=True, proj_drop=0., use_postln_in_modulation=False, normalize_modulator=False):\n        super().__init__()\n\n        self.dim = dim\n        self.focal_window = focal_window\n        self.focal_level = focal_level\n        self.focal_factor = focal_factor\n        self.use_postln_in_modulation = use_postln_in_modulation\n        self.normalize_modulator = normalize_modulator\n\n        self.f = Dense(2 * dim + (self.focal_level + 1), use_bias=bias, name=\"f\")\n        self.h = Conv2D(filters=dim, kernel_size=1, strides=1, use_bias=bias, name=\"h\")\n\n        self.act = Activation(\"gelu\")\n        self.proj = Dense(dim, name=\"proj\")\n        self.proj_drop = Dropout(proj_drop)\n        self.gap = GlobalAveragePooling2D(keepdims=True)\n        self.focal_layers = []\n\n        self.kernel_sizes = []\n        for k in range(self.focal_level):\n            kernel_size = self.focal_factor * k + self.focal_window\n            self.focal_layers.append(\n                tf.keras.Sequential([\n                    Conv2D(filters=dim, kernel_size=kernel_size, strides=1,\n                           groups=dim, padding='same', use_bias=False,\n                           activation=tf.keras.activations.gelu, dtype=tf.keras.backend.floatx())\n                ])\n            )\n            self.kernel_sizes.append(kernel_size)\n        if self.use_postln_in_modulation:\n            self.ln = LayerNormalization(epsilon=1e-5)\n\n    def call(self, x, return_modulator=False):\n        C = x.shape[-1]\n\n        # pre linear projection\n        x = self.f(x)\n        q, ctx, self.gates = tf.split(x, [C, C, self.focal_level + 1], axis=-1)\n\n        # context aggregation\n        ctx_all = 0\n        for l in range(self.focal_level):\n            ctx = tf.cast(self.focal_layers[l](ctx), dtype=ctx.dtype)\n            ctx_all = ctx_all + ctx * self.gates[..., l:l + 1]\n        ctx_global = self.act(self.gap(ctx))\n        ctx_all = ctx_all + ctx_global * self.gates[..., self.focal_level:]\n        # normalize context\n        if self.normalize_modulator:\n            ctx_all = ctx_all / (self.focal_level + 1)\n\n        # focal modulation\n        self.modulator = self.h(ctx_all)\n        x_out = q * self.modulator\n        if self.use_postln_in_modulation:\n            x_out = self.ln(x_out)\n\n        # post linear projection\n        x_out = self.proj(x_out)\n        x_out = self.proj_drop(x_out)\n        if return_modulator:\n            return x_out, self.modulator\n        return x_out\n\nclass FocalNetBlock(tf.keras.layers.Layer):\n    def __init__(self, dim, input_resolution, mlp_ratio=4., drop=0., drop_path=0.,\n                 act_layer=tf.keras.activations.gelu, norm_layer=LayerNormalization,\n                 focal_level=1, focal_window=3,\n                 use_layerscale=False, layerscale_value=1e-4,\n                 use_postln=False, use_postln_in_modulation=False,\n                 normalize_modulator=False, name=None):\n        super().__init__(name=name if name is not None else '')\n\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.mlp_ratio = mlp_ratio\n\n        self.focal_window = focal_window\n        self.focal_level = focal_level\n        self.use_postln = use_postln\n\n        self.norm1 = norm_layer(epsilon=1e-5)\n        self.modulation = FocalModulation(\n            dim, proj_drop=drop, focal_window=focal_window, focal_level=self.focal_level,\n            use_postln_in_modulation=use_postln_in_modulation, normalize_modulator=normalize_modulator\n        )\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else Identity()\n        self.norm2 = norm_layer(epsilon=1e-5)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        self.gamma_1 = 1.0\n        self.gamma_2 = 1.0\n        if use_layerscale:\n            self.gamma_1 = tf.Variable(layerscale_value * tf.ones((dim)), trainable=True, name=name + \"gamma_1\")\n            self.gamma_2 = tf.Variable(layerscale_value * tf.ones((dim)), trainable=True, name=name + \"gamma_2\")\n\n        self.H = None\n        self.W = None\n\n    def call(self, x, H=None, W=None, return_modulator=False):\n        B, L, C = x.shape\n        shortcut = x\n\n        # Focal Modulation\n        x = x if self.use_postln else self.norm1(x)\n        x = tf.reshape(x, shape=[-1, H, W, C])\n        if return_modulator:\n            x, modulator = self.modulation(x, return_modulator=return_modulator)\n        else:\n            x = self.modulation(x, return_modulator=return_modulator)\n        x = tf.reshape(x, [-1, H * W, C])\n        x = x if not self.use_postln else self.norm1(x)\n\n        # FFN\n        x = shortcut + self.drop_path(tf.cast(self.gamma_1, dtype=x.dtype) * x)\n        x = x + self.drop_path(tf.cast(self.gamma_2, dtype=x.dtype) * (self.norm2(self.mlp(x)) if self.use_postln else self.mlp(self.norm2(x))))\n\n        if return_modulator:\n            return x, modulator\n        return x\n\nclass BasicLayer(tf.keras.layers.Layer):\n    def __init__(self, dim, out_dim, input_resolution, depth,\n                 mlp_ratio=4., drop=0., drop_path=0., norm_layer=LayerNormalization,\n                 downsample=None, focal_level=1, focal_window=1,\n                 use_conv_embed=False, use_layerscale=False, layerscale_value=1e-4,\n                 use_postln=False, use_postln_in_modulation=False, normalize_modulator=False, name=None):\n        super().__init__(name=name if name is not None else '')\n\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n\n        # build blocks\n        self.blocks = [\n            FocalNetBlock(\n                dim=dim,\n                input_resolution=input_resolution,\n                mlp_ratio=mlp_ratio,\n                drop=drop,\n                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                norm_layer=norm_layer,\n                focal_level=focal_level,\n                focal_window=focal_window,\n                use_layerscale=use_layerscale,\n                layerscale_value=layerscale_value,\n                use_postln=use_postln,\n                use_postln_in_modulation=use_postln_in_modulation,\n                normalize_modulator=normalize_modulator,\n                name=f\"{name}_focalnet_block_{i}\"\n            )\n            for i in range(depth)\n        ]\n\n        if downsample is not None:\n            self.downsample = downsample(\n                img_size=input_resolution,\n                patch_size=2,\n                in_chans=dim,\n                embed_dim=out_dim,\n                use_conv_embed=use_conv_embed,\n                norm_layer=norm_layer,\n                is_stem=False\n            )\n        else:\n            self.downsample = None\n\n    def call(self, x, H=None, W=None, return_modulator=False):\n        modulators = []\n        for blk in self.blocks:\n            if return_modulator:\n                x, modulator = blk(x, H=H, W=W, return_modulator=return_modulator)\n                modulators.append(modulator)\n            else:\n                x = blk(x, H=H, W=W, return_modulator=return_modulator)\n        if self.downsample is not None:\n            x = tf.reshape(x, [-1, H, W, x.shape[-1]])\n            x, Ho, Wo = self.downsample(x)\n        else:\n            Ho, Wo = H, W\n\n        if return_modulator:\n            return x, Ho, Wo, modulators\n        return x, Ho, Wo\n\nclass FocalNet(tf.keras.Model):\n    def __init__(self,\n                 img_size=224,\n                 patch_size=4,\n                 in_chans=3,\n                 num_classes=1000,\n                 embed_dim=96,\n                 depths=[2, 2, 6, 2],\n                 mlp_ratio=4.,\n                 drop_rate=0.,\n                 drop_path_rate=0.2,\n                 norm_layer=LayerNormalization,\n                 patch_norm=True,\n                 focal_levels=[2, 2, 2, 2],\n                 focal_windows=[3, 3, 3, 3],\n                 use_conv_embed=False,\n                 use_layerscale=False,\n                 layerscale_value=1e-4,\n                 use_postln=False,\n                 use_postln_in_modulation=False,\n                 normalize_modulator=False,\n                 pooling=\"avg\",\n                 include_top=True,\n                 name=None,\n                 act_head=None):\n        super().__init__(name=name if name is not None else '')\n        if type(img_size) == int:\n            img_size = (img_size, img_size)\n        assert type(img_size) == tuple\n        self.num_layers = len(depths)\n        embed_dim = [embed_dim * (2 ** i) for i in range(self.num_layers)]\n\n        self.num_classes = num_classes\n        self.embed_dim = embed_dim\n        self.patch_norm = patch_norm\n        self.num_features = embed_dim[-1]\n        self.mlp_ratio = mlp_ratio\n\n        self.patch_embed = PatchEmbed(\n            img_size=to_2tuple(img_size),\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim[0],\n            use_conv_embed=use_conv_embed,\n            norm_layer=norm_layer if self.patch_norm else None,\n            is_stem=True\n        )\n\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n        self.pos_drop = Dropout(drop_rate)\n\n        dpr = [x for x in tf.linspace(0., drop_path_rate, sum(depths)).numpy()]\n\n        self.layers_ = []\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(\n                dim=embed_dim[i_layer],\n                out_dim=embed_dim[i_layer + 1] if (i_layer < self.num_layers - 1) else None,\n                input_resolution=(patches_resolution[0] // (2 ** i_layer),\n                                  patches_resolution[1] // (2 ** i_layer)),\n                depth=depths[i_layer],\n                mlp_ratio=self.mlp_ratio,\n                drop=drop_rate,\n                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                norm_layer=norm_layer,\n                downsample=PatchEmbed if (i_layer < self.num_layers - 1) else None,\n                focal_level=focal_levels[i_layer],\n                focal_window=focal_windows[i_layer],\n                use_conv_embed=use_conv_embed,\n                use_layerscale=use_layerscale,\n                layerscale_value=layerscale_value,\n                use_postln=use_postln,\n                use_postln_in_modulation=use_postln_in_modulation,\n                normalize_modulator=normalize_modulator,\n                name=f\"basic_layer_{i_layer}\"\n            )\n            self.layers_.append(layer)\n\n        self.norm = norm_layer(epsilon=1e-5, name=\"norm\")\n        self.avgpool = GlobalAveragePooling1D() if pooling == \"avg\" else GlobalMaxPooling1D() if pooling == \"max\" else None\n        self.head = Dense(num_classes, name=\"head\", activation=act_head, dtype=tf.float32) if num_classes > 0 and include_top else Identity()\n        self.return_modulator = False\n        self.build((1, img_size[0], img_size[1], 3))\n\n    def set_return_modulator(self, do_return=True):\n        self.return_modulator = do_return\n\n    def extract_features(self, x, return_modulator):\n        x, H, W = self.patch_embed(x)\n        x = self.pos_drop(x)\n        for layer in self.layers_:\n            if layer == self.layers_[-1] and return_modulator:\n                x, H, W, modulators = layer(x, H=H, W=W, return_modulator=return_modulator)\n            else:\n                x, H, W = layer(x, H=H, W=W, return_modulator=False)\n        x = self.norm(x)\n        if return_modulator:\n            return x, modulators\n        return x\n\n    def call(self, x):\n        if self.return_modulator:\n            x, modulators = self.extract_features(x, return_modulator=self.return_modulator)\n        else:\n            x = self.extract_features(x, return_modulator=self.return_modulator)\n        if self.avgpool is not None:\n            x = self.avgpool(x)\n        x = self.head(x)\n        if self.return_modulator:\n            return x, modulators\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-06T18:41:47.655449Z","iopub.execute_input":"2024-06-06T18:41:47.655846Z","iopub.status.idle":"2024-06-06T18:41:51.465396Z","shell.execute_reply.started":"2024-06-06T18:41:47.655815Z","shell.execute_reply":"2024-06-06T18:41:51.464509Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-06 18:41:48.080883: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-06 18:41:48.080945: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-06 18:41:48.082427: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"def focalnet_tiny_srf_tensorflow(pretrained=False, **kwargs):\n    model = FocalNet(depths=[2, 2, 6, 2], embed_dim=96, drop_path_rate=0.2, focal_levels=[2, 2, 2, 2], focal_windows=[3, 3, 3, 3], **kwargs)\n    return model\n\nmodel = focalnet_tiny_srf_tensorflow(pretrained=False)\ninput_tensor = tf.random.normal([1, 224, 224, 3])\noutput = model(input_tensor, training=False)\nn_parameters = np.sum([np.prod(v.shape) for v in model.trainable_weights])\nprint(f\"Number of Params: {n_parameters / 1000000:.1f}M\")","metadata":{"execution":{"iopub.status.busy":"2024-06-06T18:41:51.467357Z","iopub.execute_input":"2024-06-06T18:41:51.467859Z","iopub.status.idle":"2024-06-06T18:42:02.172602Z","shell.execute_reply.started":"2024-06-06T18:41:51.467833Z","shell.execute_reply":"2024-06-06T18:42:02.171596Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:361: UserWarning: `build()` was called on layer '', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1717699315.199451     472 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Number of Params: 28.4M\n","output_type":"stream"}]},{"cell_type":"code","source":"tf_model = focalnet_tiny_srf_tensorflow(pretrained=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T18:42:02.173717Z","iopub.execute_input":"2024-06-06T18:42:02.174016Z","iopub.status.idle":"2024-06-06T18:42:02.450781Z","shell.execute_reply.started":"2024-06-06T18:42:02.173991Z","shell.execute_reply":"2024-06-06T18:42:02.449897Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"for i, layer in enumerate(tf_model.layers):\n    print(i, layer.name, [w.shape for w in layer.get_weights()])","metadata":{"execution":{"iopub.status.busy":"2024-06-06T18:42:02.452100Z","iopub.execute_input":"2024-06-06T18:42:02.452486Z","iopub.status.idle":"2024-06-06T18:42:02.459741Z","shell.execute_reply.started":"2024-06-06T18:42:02.452451Z","shell.execute_reply":"2024-06-06T18:42:02.458749Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"0 patch_embed_4 []\n1 dropout_13 []\n2 basic_layer_0 []\n3 basic_layer_1 []\n4 basic_layer_2 []\n5 basic_layer_3 []\n6 norm []\n7 global_average_pooling1d_1 []\n8 head []\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Pytorch**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nfrom timm.models.registry import register_model\n\nfrom torchvision import transforms\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.data import create_transform\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)     \n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\nclass FocalModulation(nn.Module):\n    def __init__(self, dim, focal_window, focal_level, focal_factor=2, bias=True, proj_drop=0., use_postln_in_modulation=False, normalize_modulator=False):\n        super().__init__()\n\n        self.dim = dim\n        self.focal_window = focal_window\n        self.focal_level = focal_level\n        self.focal_factor = focal_factor\n        self.use_postln_in_modulation = use_postln_in_modulation\n        self.normalize_modulator = normalize_modulator\n\n        self.f = nn.Linear(dim, 2*dim + (self.focal_level+1), bias=bias)\n        self.h = nn.Conv2d(dim, dim, kernel_size=1, stride=1, bias=bias)\n\n        self.act = nn.GELU()\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n        self.focal_layers = nn.ModuleList()\n                \n        self.kernel_sizes = []\n        for k in range(self.focal_level):\n            kernel_size = self.focal_factor*k + self.focal_window\n            self.focal_layers.append(\n                nn.Sequential(\n                    nn.Conv2d(dim, dim, kernel_size=kernel_size, stride=1, \n                    groups=dim, padding=kernel_size//2, bias=False),\n                    nn.GELU(),\n                    )\n                )              \n            self.kernel_sizes.append(kernel_size)          \n        if self.use_postln_in_modulation:\n            self.ln = nn.LayerNorm(dim)\n\n    def forward(self, x):\n        C = x.shape[-1]\n\n        # pre linear projection\n        x = self.f(x).permute(0, 3, 1, 2).contiguous()\n        q, ctx, self.gates = torch.split(x, (C, C, self.focal_level+1), 1)\n        \n        # context aggreation\n        ctx_all = 0 \n        for l in range(self.focal_level):         \n            ctx = self.focal_layers[l](ctx)\n            ctx_all = ctx_all + ctx*self.gates[:, l:l+1]\n        ctx_global = self.act(ctx.mean(2, keepdim=True).mean(3, keepdim=True))\n        ctx_all = ctx_all + ctx_global*self.gates[:,self.focal_level:]\n\n        # normalize context\n        if self.normalize_modulator:\n            ctx_all = ctx_all / (self.focal_level+1)\n\n        # focal modulation\n        self.modulator = self.h(ctx_all)\n        x_out = q*self.modulator\n        x_out = x_out.permute(0, 2, 3, 1).contiguous()\n        if self.use_postln_in_modulation:\n            x_out = self.ln(x_out)\n        \n        # post linear porjection\n        x_out = self.proj(x_out)\n        x_out = self.proj_drop(x_out)\n        return x_out\n\n    def extra_repr(self) -> str:\n        return f'dim={self.dim}'\n\n    def flops(self, N):\n        # calculate flops for 1 window with token length of N\n        flops = 0\n\n        flops += N * self.dim * (self.dim * 2 + (self.focal_level+1))\n\n        # focal convolution\n        for k in range(self.focal_level):\n            flops += N * (self.kernel_sizes[k]**2+1) * self.dim\n\n        # global gating\n        flops += N * 1 * self.dim \n\n        #  self.linear\n        flops += N * self.dim * (self.dim + 1)\n\n        # x = self.proj(x)\n        flops += N * self.dim * self.dim\n        return flops\n\nclass FocalNetBlock(nn.Module):\n\n    def __init__(self, dim, input_resolution, mlp_ratio=4., drop=0., drop_path=0., \n                    act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                    focal_level=1, focal_window=3,\n                    use_layerscale=False, layerscale_value=1e-4, \n                    use_postln=False, use_postln_in_modulation=False, \n                    normalize_modulator=False):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.mlp_ratio = mlp_ratio\n\n        self.focal_window = focal_window\n        self.focal_level = focal_level\n        self.use_postln = use_postln\n\n        self.norm1 = norm_layer(dim)\n        self.modulation = FocalModulation(\n            dim, proj_drop=drop, focal_window=focal_window, focal_level=self.focal_level, \n            use_postln_in_modulation=use_postln_in_modulation, normalize_modulator=normalize_modulator\n        )\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        self.gamma_1 = 1.0\n        self.gamma_2 = 1.0    \n        if use_layerscale:\n            self.gamma_1 = nn.Parameter(layerscale_value * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(layerscale_value * torch.ones((dim)), requires_grad=True)\n\n        self.H = None\n        self.W = None\n\n    def forward(self, x):\n        H, W = self.H, self.W\n        B, L, C = x.shape\n        shortcut = x\n\n        # Focal Modulation\n        x = x if self.use_postln else self.norm1(x)\n        x = x.view(B, H, W, C)\n        x = self.modulation(x).view(B, H * W, C)\n        x = x if not self.use_postln else self.norm1(x)\n\n        # FFN\n        x = shortcut + self.drop_path(self.gamma_1 * x)\n        x = x + self.drop_path(self.gamma_2 * (self.norm2(self.mlp(x)) if self.use_postln else self.mlp(self.norm2(x))))\n\n        return x\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, \" \\\n               f\"mlp_ratio={self.mlp_ratio}\"\n\n    def flops(self):\n        flops = 0\n        H, W = self.input_resolution\n        # norm1\n        flops += self.dim * H * W\n        \n        # W-MSA/SW-MSA\n        flops += self.modulation.flops(H*W)\n\n        # mlp\n        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n        # norm2\n        flops += self.dim * H * W\n        return flops\n\nclass BasicLayer(nn.Module):\n\n    def __init__(self, dim, out_dim, input_resolution, depth,\n                 mlp_ratio=4., drop=0., drop_path=0., norm_layer=nn.LayerNorm, \n                 downsample=None, use_checkpoint=False, \n                 focal_level=1, focal_window=1, \n                 use_conv_embed=False, \n                 use_layerscale=False, layerscale_value=1e-4, \n                 use_postln=False, \n                 use_postln_in_modulation=False, \n                 normalize_modulator=False):\n\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n        \n        # build blocks\n        self.blocks = nn.ModuleList([\n            FocalNetBlock(\n                dim=dim, \n                input_resolution=input_resolution,\n                mlp_ratio=mlp_ratio, \n                drop=drop, \n                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                norm_layer=norm_layer,\n                focal_level=focal_level,\n                focal_window=focal_window, \n                use_layerscale=use_layerscale, \n                layerscale_value=layerscale_value,\n                use_postln=use_postln, \n                use_postln_in_modulation=use_postln_in_modulation, \n                normalize_modulator=normalize_modulator, \n            )\n            for i in range(depth)])\n\n        if downsample is not None:\n            self.downsample = downsample(\n                img_size=input_resolution, \n                patch_size=2, \n                in_chans=dim, \n                embed_dim=out_dim, \n                use_conv_embed=use_conv_embed, \n                norm_layer=norm_layer, \n                is_stem=False\n            )\n        else:\n            self.downsample = None\n\n    def forward(self, x, H, W):\n        for blk in self.blocks:\n            blk.H, blk.W = H, W\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n\n        if self.downsample is not None:\n            x = x.transpose(1, 2).reshape(x.shape[0], -1, H, W)\n            x, Ho, Wo = self.downsample(x)\n        else:\n            Ho, Wo = H, W        \n        return x, Ho, Wo\n\n    def extra_repr(self) -> str:\n        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n\n    def flops(self):\n        flops = 0\n        for blk in self.blocks:\n            flops += blk.flops()\n        if self.downsample is not None:\n            flops += self.downsample.flops()\n        return flops\n\nclass PatchEmbed(nn.Module):\n\n    def __init__(self, img_size=(224, 224), patch_size=4, in_chans=3, embed_dim=96, use_conv_embed=False, norm_layer=None, is_stem=False):\n        super().__init__()\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        if use_conv_embed:\n            # if we choose to use conv embedding, then we treat the stem and non-stem differently\n            if is_stem:\n                kernel_size = 7; padding = 2; stride = 4\n            else:\n                kernel_size = 3; padding = 1; stride = 2\n            self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding)\n        else:\n            self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        \n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n\n        x = self.proj(x)        \n        H, W = x.shape[2:]\n        x = x.flatten(2).transpose(1, 2)  # B Ph*Pw C\n        if self.norm is not None:\n            x = self.norm(x)\n        return x, H, W\n\n    def flops(self):\n        Ho, Wo = self.patches_resolution\n        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n        if self.norm is not None:\n            flops += Ho * Wo * self.embed_dim\n        return flops\n\nclass FocalNet(nn.Module):\n    def __init__(self, \n                img_size=224, \n                patch_size=4, \n                in_chans=3, \n                num_classes=1000,\n                embed_dim=96, \n                depths=[2, 2, 6, 2], \n                mlp_ratio=4., \n                drop_rate=0., \n                drop_path_rate=0.1,\n                norm_layer=nn.LayerNorm, \n                patch_norm=True,\n                use_checkpoint=False,                 \n                focal_levels=[2, 2, 2, 2], \n                focal_windows=[3, 3, 3, 3], \n                use_conv_embed=False, \n                use_layerscale=False, \n                layerscale_value=1e-4, \n                use_postln=False, \n                use_postln_in_modulation=False, \n                normalize_modulator=False, \n                **kwargs):\n        super().__init__()\n\n        self.num_layers = len(depths)\n        embed_dim = [embed_dim * (2 ** i) for i in range(self.num_layers)]\n\n        self.num_classes = num_classes\n        self.embed_dim = embed_dim\n        self.patch_norm = patch_norm\n        self.num_features = embed_dim[-1]\n        self.mlp_ratio = mlp_ratio\n        \n        # split image into patches using either non-overlapped embedding or overlapped embedding\n        self.patch_embed = PatchEmbed(\n            img_size=to_2tuple(img_size), \n            patch_size=patch_size, \n            in_chans=in_chans, \n            embed_dim=embed_dim[0], \n            use_conv_embed=use_conv_embed, \n            norm_layer=norm_layer if self.patch_norm else None, \n            is_stem=True)\n\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(dim=embed_dim[i_layer], \n                               out_dim=embed_dim[i_layer+1] if (i_layer < self.num_layers - 1) else None,  \n                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n                                                 patches_resolution[1] // (2 ** i_layer)),\n                               depth=depths[i_layer],\n                               mlp_ratio=self.mlp_ratio,\n                               drop=drop_rate, \n                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                               norm_layer=norm_layer, \n                               downsample=PatchEmbed if (i_layer < self.num_layers - 1) else None,\n                               focal_level=focal_levels[i_layer], \n                               focal_window=focal_windows[i_layer], \n                               use_conv_embed=use_conv_embed,\n                               use_checkpoint=use_checkpoint, \n                               use_layerscale=use_layerscale, \n                               layerscale_value=layerscale_value, \n                               use_postln=use_postln,\n                               use_postln_in_modulation=use_postln_in_modulation, \n                               normalize_modulator=normalize_modulator\n                    )\n            self.layers.append(layer)\n\n        self.norm = norm_layer(self.num_features)\n        self.avgpool = nn.AdaptiveAvgPool1d(1)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {''}\n\n    @torch.jit.ignore\n    def no_weight_decay_keywords(self):\n        return {''}\n\n    def forward_featuremaps(self, x):\n        x, H, W = self.patch_embed(x)\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x, H, W = layer(x, H, W)\n        x = self.norm(x)  # B L C\n        return x\n    \n    def forward_features(self, x):\n        x, H, W = self.patch_embed(x)\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x, H, W = layer(x, H, W)\n        x = self.norm(x)  # B L C\n        x = self.avgpool(x.transpose(1, 2))  # B C 1\n        x = torch.flatten(x, 1)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n\n    def flops(self):\n        flops = 0\n        flops += self.patch_embed.flops()\n        for i, layer in enumerate(self.layers):\n            flops += layer.flops()\n        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n        flops += self.num_features * self.num_classes\n        return flops\n\ndef build_transforms(img_size, center_crop=False):\n    t = []\n    if center_crop:\n        size = int((256 / 224) * img_size)\n        t.append(\n            transforms.Resize(size, interpolation='bicubic')\n        )\n        t.append(\n            transforms.CenterCrop(img_size)    \n        )\n    else:\n        t.append(\n            transforms.Resize(img_size, interpolation='bicubic')\n        )        \n    t.append(transforms.ToTensor())\n    t.append(transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD))\n    return transforms.Compose(t)\n\ndef build_transforms4display(img_size, center_crop=False):\n    t = []\n    if center_crop:\n        size = int((256 / 224) * img_size)\n        t.append(\n            transforms.Resize(size, interpolation='bicubic')\n        )\n        t.append(\n            transforms.CenterCrop(img_size)    \n        )\n    else:\n        t.append(\n            transforms.Resize(img_size, interpolation='bicubic')\n        )  \n    t.append(transforms.ToTensor())\n    return transforms.Compose(t)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T18:42:02.463559Z","iopub.execute_input":"2024-06-06T18:42:02.463886Z","iopub.status.idle":"2024-06-06T18:42:06.408147Z","shell.execute_reply.started":"2024-06-06T18:42:02.463859Z","shell.execute_reply":"2024-06-06T18:42:06.407117Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def focalnet_tiny_srf_pytorch(pretrained=False, **kwargs):\n    model = FocalNet(depths=[2, 2, 6, 2], embed_dim=96, focal_levels=[2, 2, 2, 2], focal_windows=[3, 3, 3, 3], **kwargs)\n    return model\n\nmodel = focalnet_tiny_srf_pytorch(pretrained=False)\nn_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Number of Params: {n_parameters / 1000000:.1f}M\")","metadata":{"execution":{"iopub.status.busy":"2024-06-06T18:42:06.409775Z","iopub.execute_input":"2024-06-06T18:42:06.410557Z","iopub.status.idle":"2024-06-06T18:42:06.932242Z","shell.execute_reply.started":"2024-06-06T18:42:06.410522Z","shell.execute_reply":"2024-06-06T18:42:06.931257Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Number of Params: 28.4M\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'focalnet_tiny_srf.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-06T18:42:06.933733Z","iopub.execute_input":"2024-06-06T18:42:06.934066Z","iopub.status.idle":"2024-06-06T18:42:07.166155Z","shell.execute_reply.started":"2024-06-06T18:42:06.934039Z","shell.execute_reply":"2024-06-06T18:42:07.165130Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"pytorch_model = focalnet_tiny_srf_pytorch(pretrained=False)\n\npytorch_model.load_state_dict(torch.load('/kaggle/working/focalnet_tiny_srf.pth'))\npytorch_model.eval()\n\n# Helper function to convert PyTorch tensors to NumPy arrays\ndef pt_to_np(tensor):\n    return tensor.detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2024-06-06T18:42:07.167623Z","iopub.execute_input":"2024-06-06T18:42:07.168001Z","iopub.status.idle":"2024-06-06T18:42:07.785142Z","shell.execute_reply.started":"2024-06-06T18:42:07.167964Z","shell.execute_reply":"2024-06-06T18:42:07.784246Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}